{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a699506-d977-4f59-9c88-37fe1f8a934d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pinky/anaconda3/envs/allenSDK_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-saved session table from: /home/pinky/PSTH_VisualData/ecephys_sessions_table.csv\n",
      "Loaded 58 session IDs from saved file.\n",
      "Main EcephysProjectCache initialized successfully for fetching session data.\n",
      "Found 58 total Ecephys sessions to process.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from allensdk.brain_observatory.ecephys.ecephys_project_cache import EcephysProjectCache # Still needed for session data\n",
    "import sys\n",
    "import io\n",
    "from contextlib import contextmanager\n",
    " \n",
    "# --- deal with verbos output ---\n",
    "@contextmanager\n",
    "def suppress_output_for_tqdm():\n",
    "    original_stdout = sys.stdout\n",
    "    original_stderr = sys.stderr\n",
    "    try:\n",
    "        # Redirect both stdout and stderr to dummy streams\n",
    "        sys.stdout = io.StringIO()\n",
    "        sys.stderr = io.StringIO()\n",
    "        yield\n",
    "    finally:\n",
    "        sys.stdout = original_stdout\n",
    "        sys.stderr = original_stderr\n",
    "\n",
    "\n",
    "        \n",
    "# --- Configuration ---\n",
    "BASE_CACHE_DIR = Path(\"/home/pinky/PSTH_VisualData/\")\n",
    "MANIFEST_PATH = str(BASE_CACHE_DIR / \"manifest.json\") # For EcephysProjectCache init\n",
    "SAVED_SESSIONS_TABLE_PATH = BASE_CACHE_DIR / \"ecephys_sessions_table.csv\" # Match saved format\n",
    "# SAVED_SESSIONS_TABLE_PATH = BASE_CACHE_DIR / \"ecephys_sessions_table.pkl\" # If saved as pickle\n",
    "\n",
    "OUTPUT_DIR = Path(\"./visual_coding_psth_output\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "VISUAL_AREAS = ['VISal', 'VISam', 'VISl', 'VISp', 'VISpm', 'VISrl']\n",
    "TARGET_STIMULUS_NAME = 'drifting_gratings'\n",
    "PRE_TIME = 0.1\n",
    "POST_TIME = 0.5\n",
    "BIN_SIZE = 0.01\n",
    "time_bins = np.arange(-PRE_TIME, POST_TIME + BIN_SIZE, BIN_SIZE)\n",
    "time_bin_centers = time_bins[:-1] + BIN_SIZE / 2\n",
    "\n",
    "# --- Load Session Table ---\n",
    "if SAVED_SESSIONS_TABLE_PATH.exists():\n",
    "    print(f\"Loading pre-saved session table from: {SAVED_SESSIONS_TABLE_PATH}\")\n",
    "    if SAVED_SESSIONS_TABLE_PATH.suffix == '.csv':\n",
    "        sessions_df = pd.read_csv(SAVED_SESSIONS_TABLE_PATH, index_col=0) # Assuming first col is index\n",
    "    elif SAVED_SESSIONS_TABLE_PATH.suffix == '.pkl':\n",
    "        sessions_df = pd.read_pickle(SAVED_SESSIONS_TABLE_PATH)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported saved sessions table format.\")\n",
    "    all_session_ids = sessions_df.index.tolist()\n",
    "    print(f\"Loaded {len(all_session_ids)} session IDs from saved file.\")\n",
    "else:\n",
    "    print(f\"Saved session table not found at {SAVED_SESSIONS_TABLE_PATH}.\")\n",
    "    print(\"Please run the script/cell to generate and save it first, or fall back to live fetching.\")\n",
    "    # Fallback to live fetching (will be slow)\n",
    "    print(\"Initializing EcephysProjectCache to fetch session table live...\")\n",
    "    try:\n",
    "        cache_for_table = EcephysProjectCache.from_warehouse(manifest=MANIFEST_PATH) # Temporary cache instance\n",
    "        sessions_df = cache_for_table.get_session_table()\n",
    "        all_session_ids = sessions_df.index.tolist()\n",
    "        print(f\"Fetched {len(all_session_ids)} session IDs live from AllenSDK.\")\n",
    "        # Optionally save it now if fetched live\n",
    "        # sessions_df.to_csv(SAVED_SESSIONS_TABLE_PATH)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching session table live: {e}\")\n",
    "        exit()\n",
    "\n",
    "\n",
    "# --- Initialize AllenSDK Cache (still needed for get_session_data) ---\n",
    "# This initialization should be faster now if manifest.json is already downloaded\n",
    "# by the script in Step 1 or a previous run.\n",
    "try:\n",
    "    cache = EcephysProjectCache.from_warehouse(manifest=MANIFEST_PATH)\n",
    "    print(\"Main EcephysProjectCache initialized successfully for fetching session data.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing main cache: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Main Processing Loop (using all_session_ids from loaded/fetched table) ---\n",
    "if not all_session_ids:\n",
    "    print(\"No session IDs to process. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Found {len(all_session_ids)} total Ecephys sessions to process.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ece9334c-af6f-48eb-97ac-59a47a29d49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 58 total Ecephys sessions.\n"
     ]
    }
   ],
   "source": [
    "# --- Get all experiment session IDs ---\n",
    "sessions_df = cache.get_session_table()\n",
    "all_session_ids = sessions_df.index.tolist()\n",
    "print(f\"Found {len(all_session_ids)} total Ecephys sessions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbc8bd39-3fe3-4a5a-97ce-da8e06e4bb47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sessions:   0%|                                                     | 0/58 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing session: 715093703\n",
      "Attempting to load/download data for session 715093703 (internal progress suppressed)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sessions:   2%|▋                                       | 1/58 [40:01<38:01:06, 2401.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading session 715093703: Download took 1200.056250861846 seconds, but timeout was set to 1200\n",
      "\n",
      "Processing session: 719161530\n",
      "Attempting to load/download data for session 719161530 (internal progress suppressed)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sessions:   3%|█▎                                    | 2/58 [1:20:02<37:21:10, 2401.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading session 719161530: Download took 1200.1675209160894 seconds, but timeout was set to 1200\n",
      "\n",
      "Processing session: 721123822\n",
      "Attempting to load/download data for session 721123822 (internal progress suppressed)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sessions:   5%|█▉                                    | 3/58 [2:00:03<36:41:13, 2401.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading session 721123822: Download took 1200.3066675253212 seconds, but timeout was set to 1200\n",
      "\n",
      "Processing session: 732592105\n",
      "Attempting to load/download data for session 732592105 (internal progress suppressed)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sessions:   7%|██▌                                   | 4/58 [2:40:05<36:01:27, 2401.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading session 732592105: Download took 1200.0301601099782 seconds, but timeout was set to 1200\n",
      "\n",
      "Processing session: 737581020\n",
      "Attempting to load/download data for session 737581020 (internal progress suppressed)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sessions:   9%|███▎                                  | 5/58 [3:20:07<35:21:17, 2401.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading session 737581020: Download took 1200.0183887421153 seconds, but timeout was set to 1200\n",
      "\n",
      "Processing session: 739448407\n",
      "Attempting to load/download data for session 739448407 (internal progress suppressed)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sessions:  10%|███▉                                  | 6/58 [4:00:08<34:41:10, 2401.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading session 739448407: Download took 1200.0037562600337 seconds, but timeout was set to 1200\n",
      "\n",
      "Processing session: 742951821\n",
      "Attempting to load/download data for session 742951821 (internal progress suppressed)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sessions:  12%|████▌                                 | 7/58 [4:40:09<34:01:06, 2401.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading session 742951821: Download took 1200.0047698239796 seconds, but timeout was set to 1200\n",
      "\n",
      "Processing session: 743475441\n",
      "Attempting to load/download data for session 743475441 (internal progress suppressed)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sessions:  14%|█████▏                                | 8/58 [5:20:10<33:21:04, 2401.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading session 743475441: Download took 1200.0169138042256 seconds, but timeout was set to 1200\n",
      "\n",
      "Processing session: 744228101\n",
      "Attempting to load/download data for session 744228101 (internal progress suppressed)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sessions:  16%|█████▉                                | 9/58 [6:00:12<32:41:02, 2401.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading session 744228101: Download took 1200.0034770970233 seconds, but timeout was set to 1200\n",
      "\n",
      "Processing session: 746083955\n",
      "Attempting to load/download data for session 746083955 (internal progress suppressed)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sessions:  17%|██████▍                              | 10/58 [6:40:13<32:01:00, 2401.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading session 746083955: Download took 1200.0053078946657 seconds, but timeout was set to 1200\n",
      "\n",
      "Processing session: 750332458\n",
      "Attempting to load/download data for session 750332458 (internal progress suppressed)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sessions:  17%|██████▍                              | 10/58 [6:57:29<33:23:55, 2504.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session 750332458 loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'units_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 37\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# 1. Filter units by visual areas\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Filter for \"good\" units within the specified visual areas\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m visual_units_df \u001b[38;5;241m=\u001b[39m \u001b[43munits_df\u001b[49m[\n\u001b[1;32m     38\u001b[0m     (units_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mecephys_structure_acronym\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(VISUAL_AREAS)) \u001b[38;5;241m&\u001b[39m \u001b[38;5;66;03m# Condition 1: In visual areas\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     (units_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquality\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgood\u001b[39m\u001b[38;5;124m'\u001b[39m)                             \u001b[38;5;66;03m# Condition 2: Unit quality is 'good'\u001b[39;00m\n\u001b[1;32m     40\u001b[0m ]\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visual_units_df\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgood\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m units found in specified visual areas for session \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msession_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Skipping.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'units_df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Main Processing Loop ---\n",
    "for session_id in tqdm(all_session_ids, desc=\"Processing Sessions\"): # Your main tqdm progress bar\n",
    "    # This print will go to the original stdout before suppression\n",
    "    print(f\"\\nProcessing session: {session_id}\")\n",
    "    output_filepath = OUTPUT_DIR / f\"session_{session_id}_psth_data.pkl\"\n",
    "\n",
    "    if output_filepath.exists():\n",
    "        # This print will go to the original stdout\n",
    "        print(f\"Data for session {session_id} already processed. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # This print will go to the original stdout\n",
    "        print(f\"Attempting to load/download data for session {session_id} (internal progress suppressed)...\")\n",
    "        \n",
    "        # --- Suppress AllenSDK's internal tqdm here (now suppressing both stdout and stderr) ---\n",
    "        with suppress_output_for_tqdm():\n",
    "            session = cache.get_session_data(session_id,\n",
    "                                             isi_violations_maximum = np.inf,\n",
    "                                             amplitude_cutoff_maximum = np.inf,\n",
    "                                             presence_ratio_minimum = -np.inf\n",
    "                                            )\n",
    "        # --- End suppression ---\n",
    "        \n",
    "        # This print will go to the original stdout\n",
    "        print(f\"Session {session_id} loaded successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # This print will go to the original stdout (or stderr if the exception handling prints there)\n",
    "        print(f\"Error loading session {session_id}: {e}\")\n",
    "        # If an error occurs, stdout/stderr are restored by the context manager's finally block\n",
    "        continue\n",
    "\n",
    "\n",
    "    # 1. Filter units by visual areas\n",
    "    # Filter for \"good\" units within the specified visual areas\n",
    "    visual_units_df = units_df[\n",
    "        (units_df['ecephys_structure_acronym'].isin(VISUAL_AREAS)) & # Condition 1: In visual areas\n",
    "        (units_df['quality'] == 'good')                             # Condition 2: Unit quality is 'good'\n",
    "    ]\n",
    "    \n",
    "    if visual_units_df.empty:\n",
    "        print(f\"No 'good' units found in specified visual areas for session {session_id}. Skipping.\")\n",
    "        # continue # Or handle as appropriate\n",
    "    else:\n",
    "        visual_unit_ids = visual_units_df.index.tolist()\n",
    "        print(f\"Found {len(visual_unit_ids)} 'good' units in visual areas.\")\n",
    "\n",
    "    # 2. Filter stimulus presentations for stationary gratings\n",
    "    stimulus_presentations_df = session.stimulus_presentations\n",
    "    \n",
    "    # Filter for the 'drifting_gratings' stimulus type\n",
    "    gratings_stim_df = stimulus_presentations_df[\n",
    "        stimulus_presentations_df['stimulus_name'] == TARGET_STIMULUS_NAME\n",
    "    ]\n",
    "\n",
    "    if gratings_stim_df.empty:\n",
    "        print(f\"No '{TARGET_STIMULUS_NAME}' stimuli found for session {session_id}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Filter for stationary gratings (temporal frequency is 0)\n",
    "    # Also, ensure relevant parameters are not NaN, as this can happen for blank sweeps\n",
    "    # or other variations sometimes included in 'drifting_gratings'\n",
    "    stationary_gratings_df = gratings_stim_df[\n",
    "        (gratings_stim_df['temporal_frequency'] == 0.0) &\n",
    "        gratings_stim_df['orientation'].notna() &\n",
    "        gratings_stim_df['spatial_frequency'].notna() &\n",
    "        gratings_stim_df['phase'].notna()\n",
    "    ].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "    if stationary_gratings_df.empty:\n",
    "        print(f\"No stationary gratings (TF=0 with valid parameters) found for session {session_id}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # 3. Create a unique stimulus information matrix\n",
    "    # These are the parameters that define a unique stationary grating stimulus\n",
    "    stimulus_params = ['orientation', 'spatial_frequency', 'phase']\n",
    "    unique_stim_conditions = stationary_gratings_df[stimulus_params].drop_duplicates().sort_values(by=stimulus_params).reset_index(drop=True)\n",
    "    unique_stim_conditions['stimulus_condition_id'] = unique_stim_conditions.index # Add an ID\n",
    "\n",
    "    if unique_stim_conditions.empty:\n",
    "        print(f\"No unique stationary grating conditions found for session {session_id}. Skipping.\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"Found {len(unique_stim_conditions)} unique stationary grating conditions.\")\n",
    "\n",
    "    # 4. Calculate PSTH for each visual unit and each unique stimulus condition\n",
    "    # Dimensions: (num_visual_units, num_unique_stim_conditions, num_time_bins)\n",
    "    all_psth_data = np.zeros((len(visual_unit_ids), len(unique_stim_conditions), len(time_bin_centers)))\n",
    "    \n",
    "    spike_times_dict = session.spike_times # More efficient to get all once\n",
    "\n",
    "    for i, unit_id in enumerate(tqdm(visual_unit_ids, desc=\"Calculating PSTHs\", leave=False)):\n",
    "        unit_spike_times = spike_times_dict.get(unit_id, np.array([]))\n",
    "        if unit_spike_times.size == 0:\n",
    "            continue # Skip if unit has no spikes\n",
    "\n",
    "        for j, stim_condition_row in unique_stim_conditions.iterrows():\n",
    "            condition_id = stim_condition_row['stimulus_condition_id']\n",
    "            \n",
    "            # Find all presentations of this specific stimulus condition\n",
    "            # Need to handle potential floating point precision issues for 'phase' and 'spatial_frequency'\n",
    "            # by comparing with a tolerance or by ensuring they were exactly the same in the table.\n",
    "            # Here, direct equality should work as they come from the same table.\n",
    "            presentations_for_condition = stationary_gratings_df[\n",
    "                (stationary_gratings_df['orientation'] == stim_condition_row['orientation']) &\n",
    "                (np.isclose(stationary_gratings_df['spatial_frequency'], stim_condition_row['spatial_frequency'])) &\n",
    "                (np.isclose(stationary_gratings_df['phase'], stim_condition_row['phase']))\n",
    "            ]\n",
    "            \n",
    "            stim_start_times = presentations_for_condition['start_time'].values\n",
    "            num_trials = len(stim_start_times)\n",
    "\n",
    "            if num_trials == 0:\n",
    "                continue # Should not happen if logic is correct, but good check\n",
    "\n",
    "            # Aggregate histograms for all trials of this condition\n",
    "            summed_histogram = np.zeros(len(time_bin_centers))\n",
    "            \n",
    "            for start_time in stim_start_times:\n",
    "                # Align spike times to this stimulus presentation's start time\n",
    "                aligned_spike_times = unit_spike_times - start_time\n",
    "                \n",
    "                # Select spikes within the PSTH window\n",
    "                spikes_in_window = aligned_spike_times[\n",
    "                    (aligned_spike_times >= -PRE_TIME) &\n",
    "                    (aligned_spike_times < POST_TIME) # Use < for the right edge of the last bin\n",
    "                ]\n",
    "                \n",
    "                # Create histogram for this trial\n",
    "                hist, _ = np.histogram(spikes_in_window, bins=time_bins)\n",
    "                summed_histogram += hist\n",
    "            \n",
    "            # Calculate mean firing rate (spikes/sec) for each bin\n",
    "            if num_trials > 0:\n",
    "                mean_firing_rate_psth = summed_histogram / (num_trials * BIN_SIZE)\n",
    "                all_psth_data[i, condition_id, :] = mean_firing_rate_psth\n",
    "\n",
    "    # 5. Save the data\n",
    "    data_to_save = {\n",
    "        'session_id': session_id,\n",
    "        'visual_unit_ids': visual_unit_ids, # List of unit IDs corresponding to 1st dim of psth_data\n",
    "        'stimulus_info': unique_stim_conditions, # DataFrame, index maps to 2nd dim of psth_data\n",
    "        'psth_data': all_psth_data, # (n_units, n_stim_conditions, n_time_bins)\n",
    "        'psth_time_bin_centers': time_bin_centers, # Time points for 3rd dim of psth_data\n",
    "        'psth_configs': {'pre_time': PRE_TIME, 'post_time': POST_TIME, 'bin_size': BIN_SIZE}\n",
    "    }\n",
    "\n",
    "    with open(output_filepath, 'wb') as f:\n",
    "        pickle.dump(data_to_save, f)\n",
    "    \n",
    "    print(f\"Saved PSTH data for session {session_id} to {output_filepath}\")\n",
    "\n",
    " # 5. *** MODIFIED: Prepare data and save as .mat file ***\n",
    "    \n",
    "    # Convert pandas DataFrame 'unique_stim_conditions_df' to a dict of arrays for MATLAB struct\n",
    "    stimulus_info_for_matlab = {}\n",
    "    for col in unique_stim_conditions_df.columns:\n",
    "        # Ensure data is in a basic type, like numpy array\n",
    "        # For object columns (like strings if 'orientation' was 'horizontal'), convert to cell array like structure\n",
    "        if unique_stim_conditions_df[col].dtype == 'object':\n",
    "             # For string arrays, convert to an object array of strings, which savemat handles as cell arrays of strings\n",
    "            stimulus_info_for_matlab[col] = np.array(unique_stim_conditions_df[col].tolist(), dtype=object)\n",
    "        else:\n",
    "            stimulus_info_for_matlab[col] = unique_stim_conditions_df[col].to_numpy()\n",
    "\n",
    "    data_to_save_mat = {\n",
    "        'session_id': str(session_id), # Ensure session_id is a string\n",
    "        'visual_unit_ids': np.array(visual_unit_ids, dtype=np.int64), # Explicitly make it a NumPy array\n",
    "        'visual_unit_areas': np.array(visual_unit_areas_list, dtype=object), # Object array for strings -> MATLAB cell array\n",
    "        'stimulus_info': stimulus_info_for_matlab, # This will become a MATLAB struct\n",
    "        'psth_data': all_psth_data, # NumPy array is fine\n",
    "        'psth_time_bin_centers': time_bin_centers, # NumPy array is fine\n",
    "        'psth_configs': { # This will become a MATLAB struct\n",
    "            'pre_time': PRE_TIME,\n",
    "            'post_time': POST_TIME,\n",
    "            'bin_size': BIN_SIZE\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        savemat(str(output_filepath_mat), data_to_save_mat, do_compression=True)\n",
    "        print(f\"Saved MAT data for session {session_id} to {output_filepath_mat}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving .mat file for session {session_id}: {e}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- All processing finished ---\")\n",
    "\n",
    "# --- Example of how to load and use the saved data ---\n",
    "# (You would typically do this in a separate script)\n",
    "#\n",
    "# output_files = list(OUTPUT_DIR.glob(\"*.pkl\"))\n",
    "# if output_files:\n",
    "#     first_file = output_files[0]\n",
    "#     print(f\"\\n--- Example: Loading data from {first_file} ---\")\n",
    "#     with open(first_file, 'rb') as f:\n",
    "#         loaded_data = pickle.load(f)\n",
    "    \n",
    "#     print(f\"Session ID: {loaded_data['session_id']}\")\n",
    "#     print(f\"Number of visual units: {len(loaded_data['visual_unit_ids'])}\")\n",
    "#     print(f\"Shape of PSTH data: {loaded_data['psth_data'].shape}\") # (units, stim_conditions, time_bins)\n",
    "#     print(\"Stimulus Information DataFrame (first 5 rows):\")\n",
    "#     print(loaded_data['stimulus_info'].head())\n",
    "#     print(\"PSTH time bin centers (first 5):\")\n",
    "#     print(loaded_data['psth_time_bin_centers'][:5])\n",
    "\n",
    "#     # Example: Plot PSTH for the first unit and first stimulus condition\n",
    "#     # import matplotlib.pyplot as plt\n",
    "#     # if loaded_data['psth_data'].size > 0 : # Check if there's actual data\n",
    "#     #     plt.figure()\n",
    "#     #     plt.plot(loaded_data['psth_time_bin_centers'], loaded_data['psth_data'][0, 0, :])\n",
    "#     #     plt.xlabel(\"Time from stimulus onset (s)\")\n",
    "#     #     plt.ylabel(\"Firing rate (spikes/s)\")\n",
    "#     #     plt.title(f\"PSTH: Unit {loaded_data['visual_unit_ids'][0]}, Stimulus Cond. 0\")\n",
    "#     #     # Get stimulus parameters for title\n",
    "#     #     stim_params_for_plot = loaded_data['stimulus_info'].iloc[0]\n",
    "#     #     plt.suptitle(f\"Ori: {stim_params_for_plot['orientation']}, SF: {stim_params_for_plot['spatial_frequency']:.2f}, Phase: {stim_params_for_plot['phase']:.2f}\")\n",
    "#     #     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d8b3bd-44df-4ead-8b0f-d906b94c3437",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
